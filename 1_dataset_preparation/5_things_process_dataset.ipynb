{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess things files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from convergence.nsd import get_resource\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(os.getenv(\"NSD_DATASET\")).parent\n",
    "things_path = dataset_path / \"things\"\n",
    "preprocessed_path = things_path / \"preprocessed\"\n",
    "preprocessed_path.mkdir(exist_ok=True)\n",
    "\n",
    "results_folder = Path(os.getenv(\"CONVERGENCE_RESULTS\"))\n",
    "\n",
    "\n",
    "filename_stimulus_metadata = \"betas_csv/betas_csv/sub-0{subject_id}_StimulusMetadata.csv\"\n",
    "filename_voxel_metadata = \"betas_csv/betas_csv/sub-0{subject_id}_VoxelMetadata.csv\"\n",
    "filename_response_data = \"/mnt/tecla/Datasets/things/betas_csv/betas_csv/sub-0{subject_id}_ResponseData.h5\"\n",
    "\n",
    "filename_betas = \"betas_vol/scalematched/sub-0{subject_id}/ses-things01/sub-0{subject_id}_ses-things01_run-01_betas.nii.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voxel_roi_table(df_voxel: pd.DataFrame, hemisphere_threshold: int) -> pd.DataFrame:\n",
    "    subject_id = df_voxel.subject_id.unique()\n",
    "    assert len(subject_id) == 1\n",
    "    subject_id = subject_id[0]\n",
    "\n",
    "    glasser_rois = [roi for roi in list(df_voxel.columns) if roi.startswith(\"glasser\")]\n",
    "    glasser_dict = {k: i+1 for i, k in enumerate(glasser_rois)}\n",
    "\n",
    "    id_vars = ['voxel_id', 'subject_id', 'voxel_x', 'voxel_y', 'voxel_z']\n",
    "\n",
    "    df_glasser = df_voxel[id_vars + glasser_rois]\n",
    "    # Pivot. Voxel x,y,z as index. columns moved to rows with a new column 'glasser' with the roi name\n",
    "    df_glasser = df_glasser.melt(id_vars=id_vars, var_name='glasser', value_name='value')\n",
    "    df_glasser = df_glasser.query('value > 0').sort_values('voxel_id').reset_index(drop=True).copy()\n",
    "    df_glasser['roi'] = df_glasser['glasser'].map(glasser_dict)\n",
    "    total_voxels = len(df_glasser)\n",
    "    assert total_voxels == len(df_glasser.drop_duplicates(subset=['voxel_x', 'voxel_y', 'voxel_z', 'roi']))\n",
    "    hcp = get_resource(\"hcp\")\n",
    "    df_glasser = df_glasser.merge(hcp[['roi', 'name']], on='roi')\n",
    "    assert total_voxels == len(df_glasser)\n",
    "    df_glasser = df_glasser.drop(columns=['glasser', 'value'])\n",
    "    df_glasser.voxel_x.astype(\"uint8\")\n",
    "    df_glasser.voxel_y.astype(\"uint8\")\n",
    "    df_glasser.voxel_z.astype(\"uint8\")\n",
    "    df_glasser.roi.astype(\"uint16\")\n",
    "    df_glasser['name'] = df_glasser['name'].astype(\"string\").astype(\"category\")\n",
    "    df_glasser.subject_id.astype(\"uint8\").astype('category')\n",
    "    \n",
    "    df_glasser.voxel_id = df_glasser.voxel_id.astype(\"uint32\")\n",
    "\n",
    "    # Add hemisphere\n",
    "    df_glasser['hemisphere'] = (df_glasser.voxel_x < hemisphere_threshold).replace({True: 'lh', False: 'rh'}).astype(\"string\").astype(\"category\")\n",
    "    df_glasser.loc[df_glasser.hemisphere == 'lh', 'roi'] += 180\n",
    "\n",
    "    assert df_glasser.voxel_id.is_unique\n",
    "    df_glasser['voxel_roi_index'] = df_glasser.groupby('roi').cumcount().astype(\"uint32\")\n",
    "    assert len(df_glasser.drop_duplicates(['roi', 'voxel_roi_index'])) == total_voxels\n",
    "\n",
    "    return df_glasser\n",
    "\n",
    "def get_voxel_size(df_voxel: pd.DataFrame) -> tuple[int, int, int]:\n",
    "    max_x = df_voxel.voxel_x.max() + 1\n",
    "    max_y = df_voxel.voxel_y.max() + 1\n",
    "    max_z = df_voxel.voxel_z.max() + 1\n",
    "    return max_x, max_y, max_z\n",
    "\n",
    "def get_dense_mask(df_roi_coordinates: pd.DataFrame, size: tuple[int, int, int]):\n",
    "    dense_mask = np.zeros(size, dtype=np.uint16)\n",
    "    for _, row in df_roi_coordinates.iterrows():\n",
    "        dense_mask[row.voxel_x, row.voxel_y, row.voxel_z] = row.roi\n",
    "    return dense_mask\n",
    "\n",
    "def compute_voxel_tables(df_stimulus, df_roi_coordinates, file_betas) -> dict[str, np.ndarray]:\n",
    "    n_trials = len(df_stimulus)\n",
    "    response_data = file_betas['ResponseData']\n",
    "    #block0_items = response_data['block0_items'][()][0] # 29431\n",
    "    voxel_ids = response_data['block1_values'][()][:, 0]\n",
    "    betas = response_data['block0_values'][()] # (211339, 9840) (n_voxels, n_trials)\n",
    "    assert betas.shape == (len(voxel_ids), n_trials)\n",
    "\n",
    "    voxel_count = df_roi_coordinates.groupby(\"roi\").voxel_roi_index.count().to_dict()\n",
    "\n",
    "\n",
    "    beta_tables = {}\n",
    "    for roi, count in voxel_count.items():\n",
    "        beta_tables[roi] = np.zeros((n_trials, count), dtype=np.float32)\n",
    "    rois_dict = df_roi_coordinates[['voxel_id', 'roi', 'voxel_roi_index']].set_index(\"voxel_id\").to_dict('index')\n",
    "\n",
    "    assert len(voxel_ids) == betas.shape[0]\n",
    "    for voxel_id, roi_betas in zip(voxel_ids, betas):\n",
    "        if voxel_id not in rois_dict:\n",
    "            continue\n",
    "        roi = rois_dict[voxel_id]['roi']\n",
    "\n",
    "        voxel_roi_index = rois_dict[voxel_id]['voxel_roi_index']\n",
    "        beta_tables[roi][:, voxel_roi_index] = roi_betas\n",
    "\n",
    "\n",
    "\n",
    "    for beta_table in beta_tables.values():\n",
    "        assert (np.abs(beta_table.sum(axis=0)) == 0).sum() == 0, \"Possible empty voxel\"\n",
    "    return beta_tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute betas by roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in tqdm([1,2,3]):\n",
    "    df_voxel = pd.read_csv(things_path / filename_voxel_metadata.format(subject_id=subject_id))\n",
    "    hemisphere_threshold = 36 #((df_voxel.voxel_x.max() + 1 ) / 2)\n",
    "    df_roi_coordinates = build_voxel_roi_table(df_voxel, hemisphere_threshold=hemisphere_threshold)\n",
    "\n",
    "    subject_folder = preprocessed_path / f\"sub-0{subject_id}\"\n",
    "    subject_folder.mkdir(exist_ok=True)\n",
    "    # Save table of roi coordinates\n",
    "    df_roi_coordinates.to_parquet(subject_folder / \"roi_coordinates.parquet\", index=False)\n",
    "    mask_size = get_voxel_size(df_voxel)\n",
    "\n",
    "    # Load an example beta map\n",
    "    beta_map = nib.load(things_path / filename_betas.format(subject_id=subject_id))\n",
    "    assert beta_map.shape[:3] == mask_size # Check that the beta map has the same size as the mask\n",
    "\n",
    "    mask = get_dense_mask(df_roi_coordinates, size=mask_size)\n",
    "\n",
    "    # Save mask as npy\n",
    "    np.save(subject_folder / \"mask-glasser.npy\", mask)\n",
    "\n",
    "    # Save mask as nii\n",
    "    mask_nii = nib.Nifti1Image(mask, beta_map.affine)\n",
    "    nib.save(mask_nii, subject_folder / \"mask-glasser.nii.gz\")\n",
    "\n",
    "    df_stimulus = pd.read_csv(things_path / filename_stimulus_metadata.format(subject_id=subject_id))\n",
    "    with h5.File(things_path / filename_response_data.format(subject_id=subject_id), 'r') as file_betas:\n",
    "        beta_tables = compute_voxel_tables(df_stimulus, df_roi_coordinates, file_betas)\n",
    "\n",
    "\n",
    "    subject_folder_betas = subject_folder / \"betas_roi\"\n",
    "    subject_folder_betas.mkdir(exist_ok=True)\n",
    "    for roi, beta_table in beta_tables.items():\n",
    "        np.save(subject_folder_betas / f\"betas_glasser_roi_{roi}.npy\", beta_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = pd.read_csv(things_path / \"stimuli/THINGS/Metadata/Concept-specific/unique_id.csv\", header=None, names=[\"category_name\"]).reset_index()\n",
    "category_ids = category_ids.rename(columns={\"index\": \"category_id\"})\n",
    "\n",
    "super_categories = pd.read_csv(things_path / \"stimuli/THINGS/27 higher-level categories/category_mat_manual.tsv\", sep=\"\\t\")\n",
    "super_categories\n",
    "\n",
    "category_ids = pd.concat([category_ids, super_categories], axis=1)\n",
    "\n",
    "filename = \"stimuli/THINGS/27 higher-level categories/categorization.tsv\"\n",
    "\n",
    "categorization = pd.read_csv(things_path / filename, sep=\"\\t\", header=None)\n",
    "\n",
    "# Join all columns into a single string concatenated by a comma\n",
    "categorization = categorization.apply(lambda x: \";\".join(set(x.dropna())), axis=1)\n",
    "category_ids['tags'] = categorization\n",
    "category_ids\n",
    "category_ids.to_csv(preprocessed_path / \"categories.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = things_path / \"stimuli/THINGS/Images\"\n",
    "images = list(images_folder.glob(\"**/*.jpg\"))\n",
    "# Sort images by name\n",
    "images.sort(key=lambda x: x.name)\n",
    "\n",
    "df_images = []\n",
    "for things_id, image in enumerate(images):\n",
    "    df_images.append(\n",
    "        {\"things_id\": things_id, \n",
    "         \"image_name\": image.stem,\n",
    "         \"category\": image.parent.name,\n",
    "         \"image_path\": str(image.relative_to(things_path)),\n",
    "        }\n",
    "    )\n",
    "df_images = pd.DataFrame(df_images)\n",
    "categories = category_ids[['category_name', 'category_id']].set_index(\"category_name\").to_dict(\"index\")\n",
    "df_images['category_id'] = df_images['category'].map(lambda x: categories[x]['category_id'])\n",
    "df_images[\"image_category_index\"] = df_images.groupby(\"category_id\").cumcount() + 1\n",
    "df_images = df_images[['things_id', 'image_name', 'category', 'category_id', 'image_category_index', 'image_path']]\n",
    "# Save images table\n",
    "df_images.to_csv(preprocessed_path / \"images.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ids = df_images[['image_name', 'things_id', 'category_id', 'category']].set_index(\"image_name\").to_dict(\"index\")\n",
    "df_captions = pd.read_csv(results_folder / \"captions/captions-revised-things-pixtral-12b.csv\")\n",
    "df_captions['things_id_2'] = df_captions['name'].map(lambda x: images_ids[x]['things_id'])\n",
    "df_captions['category_id'] = df_captions['name'].map(lambda x: images_ids[x]['category_id'])\n",
    "df_captions['category'] = df_captions['name'].map(lambda x: images_ids[x]['category'])\n",
    "df_captions = df_captions[['things_id_2', 'name', 'category', 'category_id', 'caption']]\n",
    "df_captions = df_captions.rename(columns={\"things_id_2\": \"things_id\"})\n",
    "df_captions.caption = df_captions.caption.astype(\"string\")\n",
    "df_captions.things_id = df_captions.things_id.astype(\"uint16\")\n",
    "df_captions.name = df_captions.name.astype(\"string\")\n",
    "df_captions.category = df_captions.category.astype(\"string\").astype(\"category\")\n",
    "df_captions.category_id = df_captions.category_id.astype(\"uint8\")\n",
    "df_captions.to_csv(preprocessed_path / \"captions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulus index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_stimulus_index = []\n",
    "for subject_id in [1,2,3]:\n",
    "    df_stimulus = pd.read_csv(things_path / filename_stimulus_metadata.format(subject_id=subject_id))\n",
    "\n",
    "    # Remove last .jpg from stimulus column\n",
    "    df_stimulus.stimulus = df_stimulus.stimulus.str.replace(\".jpg\", \"\")\n",
    "    # Rename stimulus column to image_name\n",
    "    df_stimulus = df_stimulus.rename(columns={\"stimulus\": \"image_name\"})\n",
    "    total_trials = len(df_stimulus)\n",
    "\n",
    "    df_stimulus = df_stimulus.merge(df_images[['things_id', 'image_name', 'category_id', 'category', 'image_category_index']], on='image_name')\n",
    "    assert total_trials == len(df_stimulus)\n",
    "\n",
    "    df_stimulus['session_index'] = df_stimulus.groupby('session').cumcount()\n",
    "    df_stimulus['run_index'] = df_stimulus.groupby('run').cumcount()\n",
    "    df_stimulus  = df_stimulus.rename(columns={'trial_id': 'subject_index'})\n",
    "    df_stimulus['repetition'] = df_stimulus.groupby('image_name').cumcount() + 1\n",
    "    df_stimulus = df_stimulus[['subject_id', 'session', 'run', 'things_id', 'subject_index', 'session_index', 'run_index', 'repetition', 'image_name', 'category_id', 'category', 'image_category_index', 'trial_type']]\n",
    "    df_stimulus_index.append(df_stimulus)\n",
    "df_stimulus_index = pd.concat(df_stimulus_index)\n",
    "df_stimulus_index.subject_id = df_stimulus_index.subject_id.astype(\"uint8\")\n",
    "df_stimulus_index.session = df_stimulus_index.session.astype(\"uint8\")\n",
    "df_stimulus_index.run = df_stimulus_index.run.astype(\"uint8\")\n",
    "df_stimulus_index.things_id = df_stimulus_index.things_id.astype(\"uint16\")\n",
    "df_stimulus_index.subject_index = df_stimulus_index.subject_index.astype(\"uint16\")\n",
    "df_stimulus_index.session_index = df_stimulus_index.session_index.astype(\"uint16\")\n",
    "df_stimulus_index.run_index = df_stimulus_index.run_index.astype(\"uint16\")\n",
    "df_stimulus_index.repetition = df_stimulus_index.repetition.astype(\"uint8\")\n",
    "df_stimulus_index.image_name = df_stimulus_index.image_name.astype(\"string\")\n",
    "df_stimulus_index.category_id = df_stimulus_index.category_id.astype(\"uint16\")\n",
    "df_stimulus_index.category = df_stimulus_index.category.astype(\"string\").astype(\"category\")\n",
    "df_stimulus_index.image_category_index = df_stimulus_index.image_category_index.astype(\"uint8\")\n",
    "df_stimulus_index.trial_type = df_stimulus_index.trial_type.astype(\"string\").astype(\"category\")\n",
    "df_stimulus_index = df_stimulus_index.reset_index(drop=True)\n",
    "\n",
    "df_stimulus_index.to_parquet(preprocessed_path / \"stimulus_index.parquet\", index=False)\n",
    "df_stimulus_index.to_csv(preprocessed_path / \"stimulus_index.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
