{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from convergence.bold5000 import get_resource, load_dataset, load_mask, get_betas_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(query=\"stim_source=='imagenet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_path = Path(\"/mnt/tecla/Datasets/bold5000\")\n",
    "presentation_list = bold_path / \"bold5000_stimuli/Stimuli_Presentation_Lists\"\n",
    "\n",
    "preprocessed_folder = bold_path / \"preprocessed\"\n",
    "preprocessed_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for subject_folder in sorted(presentation_list.glob(\"CSI*\"), key=lambda x: x.name):\n",
    "    if subject_folder.is_file(): continue\n",
    "    subject = int(subject_folder.name.replace(\"CSI\", \"\"))\n",
    "    for session_folder in sorted(subject_folder.glob(\"CSI*_sess*\"), key=lambda x: x.name):\n",
    "        if session_folder.is_file(): continue\n",
    "        session = int(session_folder.name.replace(\"CSI{}_sess\".format(subject), \"\"))\n",
    "        for run_file in sorted(session_folder.glob(\"*.txt\"), key=lambda x: x.name):\n",
    "            run = int(run_file.stem[-2:])\n",
    "            run_index = 0\n",
    "            with open(run_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for stim in lines:\n",
    "                stim = stim.strip()\n",
    "                if not stim: continue\n",
    "                \n",
    "\n",
    "                # If stim startswith rep_, remove it\n",
    "                repeated = False\n",
    "                if stim.startswith('rep_'):\n",
    "                    stim = stim[4:]\n",
    "                    repeated = True\n",
    "\n",
    "                if stim.startswith('COCO'):\n",
    "                    stim_source = 'coco'\n",
    "                # Elif is n\\d+_\\d+.* \n",
    "                elif re.match(r\"n\\d+_\\d+.*\", stim):\n",
    "                    stim_source = 'imagenet'\n",
    "                else:\n",
    "                    stim_source = 'scenes'\n",
    "                    \n",
    "                data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"session\": session,\n",
    "                    \"run\": run,\n",
    "                    \"run_index\": run_index,\n",
    "                    \"image_name\": stim,\n",
    "                    \"stim_source\": stim_source,\n",
    "                    \"repeated\": repeated\n",
    "                })\n",
    "                run_index += 1\n",
    "\n",
    "df_stim = pd.DataFrame(data)\n",
    "# Sort by subject, session and run\n",
    "df_stim = df_stim.sort_values(by=[\"subject\", \"session\", \"run\", \"run_index\"])\n",
    "df_stim = df_stim.reset_index(drop=True)\n",
    "# Create a session_index and a subject_index with the position of each stim inside the session or inside the full subject\n",
    "df_stim[\"session_index\"] = df_stim.groupby([\"subject\", \"session\"]).cumcount()\n",
    "df_stim[\"subject_index\"] = df_stim.groupby([\"subject\"]).cumcount()\n",
    "\n",
    "# Add repetition index by cumsum per subject and image_name\n",
    "df_stim[\"repetition\"] = df_stim.groupby([\"subject\", \"image_name\"]).cumcount() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stim_images = df_stim.copy()[['image_name', 'stim_source']]\n",
    "df_stim_images = df_stim_images.drop_duplicates(subset=['image_name'])\n",
    "df_stim_images = df_stim_images.reset_index()\n",
    "df_stim_images = df_stim_images.rename(columns={'index': 'bold_id'})\n",
    "\n",
    "\n",
    "presented_stimuli_folder = bold_path / \"bold5000_stimuli/Scene_Stimuli/Presented_Stimuli\"\n",
    "dataset_folders = {'coco': presented_stimuli_folder / 'COCO', 'imagenet': presented_stimuli_folder / 'ImageNet', 'scenes': presented_stimuli_folder / 'Scene'}\n",
    "\n",
    "df_stim_images['image_folder'] = df_stim_images['stim_source'].map(dataset_folders)\n",
    "df_stim_images['image_path'] = df_stim_images.apply(lambda x: x['image_folder'] / x['image_name'], axis=1)\n",
    "df_stim_images['exists'] = df_stim_images.image_path.apply(lambda x: x.exists())\n",
    "assert df_stim_images.exists.all(), \"Some images are missing\"\n",
    "# Drop\n",
    "df_stim_images.image_path = df_stim_images.image_path.apply(lambda x: str(x.relative_to(bold_path)))\n",
    "df_stim_images = df_stim_images.drop(columns=['image_folder', 'exists'])\n",
    "df_stim_images['bold_id'] = df_stim_images['bold_id'].astype(\"uint16\")\n",
    "df_stim_images['image_name'] = df_stim_images['image_name'].astype(\"string\")\n",
    "df_stim_images['stim_source'] = df_stim_images['stim_source'].astype(\"string\").astype(\"category\")\n",
    "df_stim_images['image_path'] = df_stim_images['image_path'].astype(\"string\")\n",
    "df_stim_images.to_csv(preprocessed_folder / \"images.csv\", index=False)\n",
    "df_stim_images.to_parquet(preprocessed_folder / \"images.parquet\", index=False)\n",
    "df_stim_images.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_ids = df_stim_images[['bold_id', 'image_name']]\n",
    "df_stim_ids = df_stim.merge(images_ids, on='image_name', validate='many_to_one')\n",
    "df_stim_ids['name'] = df_stim_ids['image_name'].str.split(\".\").str[0].str.lower()\n",
    "df_stim_ids = df_stim_ids[['subject', 'session', 'run', 'bold_id', 'subject_index', 'session_index', 'run_index', 'repetition', 'name', 'stim_source', 'image_name']]\n",
    "df_stim_ids['subject'] = df_stim_ids['subject'].astype('uint8')\n",
    "df_stim_ids['session'] = df_stim_ids['session'].astype('uint8')\n",
    "df_stim_ids['run'] = df_stim_ids['run'].astype('uint8')\n",
    "df_stim_ids['bold_id'] = df_stim_ids['bold_id'].astype('uint32')\n",
    "df_stim_ids['subject_index'] = df_stim_ids['subject_index'].astype('uint16')\n",
    "df_stim_ids['session_index'] = df_stim_ids['session_index'].astype('uint16')\n",
    "df_stim_ids['run_index'] = df_stim_ids['run_index'].astype('uint8')\n",
    "df_stim_ids['repetition'] = df_stim_ids['repetition'].astype('uint8')\n",
    "df_stim_ids['name'] = df_stim_ids['name'].astype('string')\n",
    "df_stim_ids['image_name'] = df_stim_ids['image_name'].astype('string')\n",
    "df_stim_ids['stim_source'] = df_stim_ids['stim_source'].astype('string').astype('category')\n",
    "df_stim_ids.to_csv(preprocessed_folder / 'stimulus_index.csv', index=False)\n",
    "df_stim_ids.to_parquet(preprocessed_folder / 'stimulus_parquet.csv', index=False)\n",
    "df_stim_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check masks processed by fsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in [1,2,3,4]:\n",
    "    mask = load_mask(subject=subject)\n",
    "    print(mask.shape)\n",
    "    assert len(np.unique(mask)) == 361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_session_betas(subject, session, betas_type=\"D\"):\n",
    "    betas_types_map = {\n",
    "        \"A\": \"CSI{subject}_GLMbetas-TYPEA-ASSUMEHRF_ses-{session}.nii.gz\",\n",
    "        \"B\": \"CSI{subject}_GLMbetas-TYPEB-FITHRF_ses-{session}.nii.gz\",\n",
    "        \"C\": \"CSI{subject}_GLMbetas-TYPEC-FITHRF-GLMDENOISE_ses-{session}.nii.gz\",\n",
    "        \"D\": \"CSI{subject}_GLMbetas-TYPED-FITHRF-GLMDENOISE-RR_ses-{session}.nii.gz\",\n",
    "    }\n",
    "    # Convert session to 2 digits with one leading 0\n",
    "    session = f\"{session:02d}\"\n",
    "    betas_type = betas_types_map[betas_type].format(subject=subject, session=session)\n",
    "    betas_path = bold_path / \"bold5000\" / betas_type\n",
    "    image = nib.load(str(betas_path))\n",
    "    data = image.get_fdata()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_folder = preprocessed_folder / \"betas\"\n",
    "betas_folder.mkdir(exist_ok=True)\n",
    "\n",
    "df_stim = get_resource(\"stimulus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta_type in [\"D\", \"C\", \"B\", \"A\"]:\n",
    "    for subject in [1,2,3,4]:\n",
    "        subject_betas_folder = betas_folder / f\"sub-{subject}\"\n",
    "        subject_betas_folder.mkdir(exist_ok=True)\n",
    "        mask = load_mask(subject=subject)\n",
    "        roi_ids, voxel_counts = np.unique(mask.ravel(), return_counts=True)\n",
    "        df_stim_subject = df_stim[df_stim.subject == subject]\n",
    "        n_stim = len(df_stim_subject)\n",
    "\n",
    "        betas_data = {}\n",
    "        for roi, count in zip(roi_ids, voxel_counts):\n",
    "            if roi == 0: continue\n",
    "            betas_data[roi] = np.zeros((n_stim, count))\n",
    "        sessions = list(df_stim_subject.session.unique())\n",
    "        sessions.sort()\n",
    "        stimulus_index = 0 \n",
    "        # Fill with roi betas\n",
    "        for session in tqdm(sessions):\n",
    "            session_betas = whole_session_betas(subject=subject, session=session, betas_type=beta_type) # (x,y,z,trials)\n",
    "            n_trials_session = session_betas.shape[-1]\n",
    "            # Fill nans with zeros\n",
    "            session_betas = np.nan_to_num(session_betas, nan=0, posinf=0, neginf=0)\n",
    "            for roi in range(1, 361):\n",
    "                betas_session_roi = session_betas[mask == roi, :]\n",
    "                betas_data[roi][stimulus_index:stimulus_index+n_trials_session, :] = betas_session_roi.T\n",
    "            stimulus_index += n_trials_session\n",
    "\n",
    "        for roi, betas in betas_data.items():\n",
    "            betas_path = subject_betas_folder / f\"sub-{subject}_hcpmmp_roi{roi}_{beta_type}.npy\"\n",
    "            np.save(betas_path, betas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
