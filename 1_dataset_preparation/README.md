# Dataset(s) Preparation

This folder contains scripts and instructions to organize datasets used throughout the project.
It includes downloading fMRI betas, stimulus images, metadata, and generating trial and region indexes. Ensure that you have at least 2TB of free space to download and 
store the processed datasets.

## Files summary

| Script Path                                                                                | Script Name                     | Short Description                                                                                              |
| ------------------------------------------------------------------------------------------ | ------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| [1\_nsd\_check\_dataset.py](./1_nsd_check_dataset.py)                                      | NSD integrity check             | Verifies required NSD files (stimuli, betas, masks, metadata) are present and correctly located.               |
| [2\_nsd\_create\_indexes.py](./2_nsd_create_indexes.py)                                    | NSD index builder               | Unrolls stimuli/trials and creates image, trial, and beta indices used by downstream RSA scripts.              |
| [3\_create\_coco\_indexes.py](./3_create_coco_indexes.py)                                  | MSâ€‘COCO index generator         | Downloads MSâ€‘COCO captions/annotations and builds trial/content indices aligned with NSD images.               |
| [4\_nsd\_organize\_betas.py](./4_nsd_organize_betas.py)                                    | NSD beta organizer              | Repackages voxelwise betas into ROIâ€wise NumPy matrices for fast loading (`n_trials Ã— n_voxels`).              |
| [5\_things\_process\_dataset.ipynb](./5_things_process_dataset.ipynb)                      | THINGS preprocessing notebook   | Organizes THINGS fMRI betas into ROI matrices and prepares auxiliary metadata (captions, categories, indices). |
| [6\_bold5000\_process\_dataset.ipynb](./6_bold5000_process_dataset.ipynb)                  | BOLD5000 preprocessing notebook | Extracts ROI betas using subjectâ€‘specific HCP atlases and builds stimulus/metadata indices for BOLD5000.       |
| [bold5000\_hcp\_atlas/mni\_to\_functional.sh](./bold5000_hcp_atlas/mni_to_functional.sh)   | MNIâ†’functional warp             | Warps the HCPâ€‘MMP1 atlas from MNI space into each BOLD5000 subjectâ€™s functional space using FSL.               |


---

## Natural Scenes Dataset (NSD)

To prepare NSD for analysis:

1. Set the environment variable pointing to the NSD data folder:

```bash
export NSD_DATASET="/your/path/to/Datasets/nsd"
```

2. After downloading the dataset (follow download instructions below), 
run the script to check file presence:

```bash
python 1_nsd_check_dataset.py
```

It will validate the presence of:

* `nsd_stimuli.hdf5`: The image dataset
* `nsd_stim_info_merged.csv`: Trial-level metadata
* Subject-specific betas and masks (e.g., `betas_session01.nii.gz`, `HCP_MMP1.nii.gz`, etc.)

3. Process the dataset using the following scripts:

* [`2_nsd_create_indexes.py`](2_nsd_create_indexes.py): Unroll images and create trial/image/beta indexes.
* [`3_create_coco_indexes.py`](3_create_coco_indexes.py): Download MS-COCO and generate caption and content trials.
* [`4_nsd_organize_betas.py`](4_nsd_organize_betas.py): Reorganize betas into ROI-specific matrices.

---

### Final Folder Structure

After completing the setup and processing steps, your `$NSD_DATASET` should look like:

```
$NSD_DATASET/
â”œâ”€â”€ subj01/
â”‚   â””â”€â”€ func1mm/
â”‚       â”œâ”€â”€ betas_fithrf_GLMdenoise_RR/
â”‚       â”‚   â”œâ”€â”€ betas_session01.nii.gz
â”‚       â”‚   â”œâ”€â”€ ...
â”‚       â”‚   â””â”€â”€ betas_session40.nii.gz
â”‚       â”œâ”€â”€ roi/
â”‚       â”‚   â”œâ”€â”€ both.HCP_MMP1.nii.gz
â”‚       â”‚   â”œâ”€â”€ lh.HCP_MMP1.nii.gz
â”‚       â”‚   â””â”€â”€ rh.HCP_MMP1.nii.gz
â”‚       â””â”€â”€ brainmask.nii.gz
â”œâ”€â”€ subj02/
â”œâ”€â”€ ...
â”œâ”€â”€ subj08/
â”œâ”€â”€ betas/
â”‚   â”œâ”€â”€ sub01/
â”‚   â”‚   â”œâ”€â”€ sub-01_roi-001.npy
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ sub-01_roi-360.npy
â”‚   â”œâ”€â”€ sub02/
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ sub08/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ nsd_00000_00999/
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ nsd_72000_72999/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ coco_captions.csv
â”‚   â”œâ”€â”€ coco_objects_annotations.csv
â”‚   â””â”€â”€ coco_persons_annotations.csv
â”œâ”€â”€ info/
â”‚   â””â”€â”€ hcp.csv
â”œâ”€â”€ nsd.csv
â”œâ”€â”€ nsd_masks.csv
â”œâ”€â”€ nsd_stimuli.csv
â”œâ”€â”€ nsd_stimuli.hdf5
â””â”€â”€ nsd_stim_info_merged.csv
```

> âœ… You can skip beta processing and use the prepared index files in `derivatives/datasets/nsd`
> by setting `NSD_DATASET=/your/path/to/derivatives/datasets/nsd`, so you can run part of the
> pipeline that do not need to access the images or the fmri data directly.


### Downloading the NSD Dataset

We use the [Natural Scenes Dataset (NSD)](https://naturalscenesdataset.org/) to compare model and brain representations.

### Step 1: Request Access

1. Visit [naturalscenesdataset.org](https://naturalscenesdataset.org/) and sign the **NSD Data Access Agreement**.
2. Youâ€™ll receive a link to the Data Manual and instructions to download via AWS.

### Step 2: Download with AWS CLI

Install the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and ensure at least 1 TB of disk space.

#### 2.1 Download Beta Files (â‰ˆ737 GB)

```bash
export NSD_DATASET="/path/to/local/folder/"

aws s3 sync s3://natural-scenes-dataset/nsddata_betas/ppdata $NSD_DATASET \
  --exclude "*" \
  --include "subj*/func1mm/betas_fithrf_GLMdenoise_RR/betas_session*.nii.gz" \
  --no-sign-request
```

#### 2.2 Download ROI and Brain Masks

```bash
aws s3 sync s3://natural-scenes-dataset/nsddata/ppdata/ $NSD_DATASET \
  --exclude "*" \
  --include "subj*/func1mm/roi/*HCP_MMP1.nii.gz" \
  --include "subj*/func1mm/brainmask.nii.gz" \
  --no-sign-request
```

#### 2.3 Download Stimuli and Trial Info

```bash
# Stimuli (37GB)
aws s3 cp s3://natural-scenes-dataset/nsddata_stimuli/stimuli/nsd/nsd_stimuli.hdf5 $NSD_DATASET \
  --no-sign-request

# Trial metadata
aws s3 cp s3://natural-scenes-dataset/nsddata/experiments/nsd/nsd_stim_info_merged.csv $NSD_DATASET \
  --no-sign-request
```

## THINGS-fMRI

To use the THINGS dataset, first define the dataset location:

```bash
export THINGS_DATASET="/path/to/your/Dataset/things"
```

### 1. Download fMRI Preprocessed Data

Download the **`betas_csv.zip`** file containing single-trial response data from the THINGS teamâ€™s Figshare repository:

ðŸ“¦ [Direct link to file](https://plus.figshare.com/articles/dataset/THINGS-data_fMRI_Single_Trial_Responses_table_format_/20492835?file=43635873)
ðŸ“š DOI: [10.25452/figshare.plus.c.6161151](https://doi.org/10.25452/figshare.plus.c.6161151)

Unzip the file inside your `$THINGS_DATASET` folder.

### 2. Process fMRI Data

Run the notebook:

```bash
things_process_dataset.ipynb
```

This notebook will:

* Organize beta values per ROI
* Create one `.npy` file per ROI of shape `[n_trials Ã— n_voxels]`

Resulting files will be created in:

```
$THINGS_DATASET/preprocessed/sub-0X/betas_roi/betas_glasser_roi_Y.npy
```

Where `X âˆˆ [1, 4]` (subjects) and `Y âˆˆ [1, 360]` (Glasser ROIs).

### 3. Organize Metadata Files

Move the following files into the `preprocessed/` directory:

* `captions.csv` â€“ Captions for each image
* `categories.csv` â€“ Image categories
* `hcp.csv` â€“ Atlas information
* `images.csv` â€“ Image filenames and paths
* `models-info.csv` â€“ Info on model responses (if applicable)
* `stimulus_index.csv` / `.parquet` â€“ Mapping from subject trials to matrix rows

> âœ… If you do not need raw betas, you can skip downloading and set:
>
> ```bash
> export THINGS_DATASET="derivatives/datasets/things"
> ```
>
> All required files are provided in that folder.

### 4. Download the Stimuli Images

Download the full THINGS image set from the OSF repository:

ðŸ”— [THINGS stimuli (OSF)](https://osf.io/jum2f/files/osfstorage)

Unzip the folder, and move it to:

```
$THINGS_DATASET/stimuli/THINGS/Images/
```

The final image paths should follow this format:

```
$THINGS_DATASET/stimuli/THINGS/Images/object_images_X_Z/<category>/<category>_XX.jpg
```

Example:

```
/path/to/things/stimuli/THINGS/Images/object_images_A-C/aardvark/aardvark_01b.jpg
```

---

### Final Folder Structure

Once all steps are complete, your THINGS dataset should look like:

```
$THINGS_DATASET/
â”œâ”€â”€ preprocessed/
â”‚   â”œâ”€â”€ captions.csv
â”‚   â”œâ”€â”€ categories.csv
â”‚   â”œâ”€â”€ hcp.csv
â”‚   â”œâ”€â”€ images.csv
â”‚   â”œâ”€â”€ models-info.csv
â”‚   â”œâ”€â”€ stimulus_index.csv
â”‚   â”œâ”€â”€ stimulus_index.parquet
â”‚   â”œâ”€â”€ sub-01/
â”‚   â”‚   â”œâ”€â”€ betas_roi/
â”‚   â”‚   â”‚   â”œâ”€â”€ betas_glasser_roi_1.npy
â”‚   â”‚   â”‚   â””â”€â”€ betas_glasser_roi_360.npy
â”‚   â”‚   â”œâ”€â”€ mask-glasser.nii.gz
â”‚   â”‚   â”œâ”€â”€ mask-glasser.npy
â”‚   â”‚   â””â”€â”€ roi_coordinates.parquet
â”‚   â”œâ”€â”€ sub-02/
â”‚   â””â”€â”€ sub-03/
â””â”€â”€ stimuli/
    â””â”€â”€ THINGS/
        â””â”€â”€ Images/
            â”œâ”€â”€ object_images_A-C/
            â”œâ”€â”€ object_images_D-K/
            â”œâ”€â”€ object_images_L-Q/
            â”œâ”€â”€ object_images_R-S/
            â””â”€â”€ object_images_T-Z/
```


## BOLD5000

To use the BOLD5000 dataset, first define the dataset path in your environment:

```bash
export BOLD_DATASET="/path/to/datasets/bold5000"
```

If you do **not** require raw betas or image stimuli, you can use only the preprocessed metadata and alignment indexes by pointing to the derivatives folder:

```bash
export BOLD_DATASET="derivatives/datasets/bold5000"
```

---

### 1. Download Stimuli

Download the stimuli images:

ðŸ“¦ [BOLD5000\_Stimuli.zip](https://www.dropbox.com/s/5ie18t4rjjvsl47/BOLD5000_Stimuli.zip?dl=1)

Unzip the contents into your `$BOLD_DATASET` folder.

---

### 2. Download fMRI Betas

Get the preprocessed beta data from the official Figshare repository (Release 2):

ðŸ”— [BOLD5000 Release 2.0 â€“ Figshare](https://figshare.com/articles/dataset/BOLD5000_Release_2_0/14456124)

---

### 3. Download Structural (T1) Files

For preprocessing and alignment, download the T1-weighted anatomical scans from:

ðŸ”— [BOLD5000 Release 1.0 â€“ OpenNeuro](https://openneuro.org/datasets/ds001499/versions/1.3.0)

---

### 4. Generate HCP ROIs in Subject Space

Youâ€™ll need to project the HCP-MMP1 atlas into the native functional space of each subject.

#### Step 1: Download the volumetric MNI atlas

ðŸ”— [HCP-MMP1 projected on MNI2009a (NIfTI)](https://figshare.com/articles/dataset/HCP-MMP1_0_projected_on_MNI2009a_GM_volumetric_in_NIfTI_format/3501911)

#### Step 2: Run the alignment script

Use FSL to warp the MNI atlas into each subjectâ€™s native space:

```bash
bash bold5000_hcp_atlas/mni_to_functional.sh
```

This will generate one `.nii.gz` file per subject:

```
$BOLD_DATASET/preprocessed/atlas/
â”œâ”€â”€ subj-01_HCP_atlas_func_360.nii.gz
â”œâ”€â”€ subj-02_HCP_atlas_func_360.nii.gz
â”œâ”€â”€ subj-03_HCP_atlas_func_360.nii.gz
â””â”€â”€ subj-04_HCP_atlas_func_360.nii.gz
```

---

### 5. Process Dataset

Run the notebook:

```bash
bold5000_process_dataset.ipynb
```

This will use the subject-specific HCP atlases to extract beta values per ROI and store them as numpy matrices of shape `[n_trials Ã— n_voxels]`.

---

### Final Folder Structure

After completing all steps, your folder should look like this:

```
$BOLD_DATASET/
â”œâ”€â”€ bold5000_stimuli/
â”‚   â””â”€â”€ Scene_Stimuli/
â”‚       â””â”€â”€ Presented_Stimuli/
â”‚           â”œâ”€â”€ COCO/
â”‚           â”‚   â”œâ”€â”€ COCO_train2014_000000000036.jpg
â”‚           â”‚   â””â”€â”€ ...
â”‚           â”œâ”€â”€ ImageNet/
â”‚           â”‚   â”œâ”€â”€ n01440764_10110.JPEG
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ Scene/
â”‚               â”œâ”€â”€ airplanecabin1.jpg
â”‚               â””â”€â”€ ...
â”œâ”€â”€ preprocessed/
â”‚   â”œâ”€â”€ atlas/
â”‚   â”‚   â”œâ”€â”€ subj-01_HCP_atlas_func_360.nii.gz
â”‚   â”‚   â”œâ”€â”€ subj-02_HCP_atlas_func_360.nii.gz
â”‚   â”‚   â”œâ”€â”€ subj-03_HCP_atlas_func_360.nii.gz
â”‚   â”‚   â””â”€â”€ subj-04_HCP_atlas_func_360.nii.gz
â”‚   â”œâ”€â”€ betas/
â”‚   â”‚   â”œâ”€â”€ sub-1/
â”‚   â”‚   â”‚   â”œâ”€â”€ sub-1_hcpmmp_roi1_A.npy
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â””â”€â”€ sub-1_hcpmmp_roi360_D.npy
â”‚   â”‚   â”œâ”€â”€ sub-2/
â”‚   â”‚   â”œâ”€â”€ sub-3/
â”‚   â”‚   â””â”€â”€ sub-4/
â”‚   â”œâ”€â”€ captions.csv
â”‚   â”œâ”€â”€ images.csv
â”‚   â”œâ”€â”€ images.parquet
â”‚   â”œâ”€â”€ stimulus_index.csv
â”‚   â””â”€â”€ stimulus_index.parquet
```
