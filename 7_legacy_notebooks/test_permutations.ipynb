{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_upper_triangle_index_matrix(n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns an (n x n) matrix 'index_matrix' with:\n",
    "      index_matrix[r, c] = k  if r < c\n",
    "      index_matrix[r, c] = -1 if r >= c\n",
    "    where k matches the ordering of torch.triu_indices(n,n,offset=1).\n",
    "    \"\"\"\n",
    "    index_matrix = torch.full((n, n), -1, dtype=torch.long)\n",
    "    # (row, col) pairs for r < c, in the same order as square_to_flat\n",
    "    r, c = torch.triu_indices(n, n, offset=1)  # length m = n*(n-1)//2\n",
    "    # Fill those positions with 0..m-1\n",
    "    index_matrix[r, c] = torch.arange(r.numel(), dtype=torch.long)\n",
    "    return index_matrix\n",
    "\n",
    "\n",
    "def permutation_to_flat_via_index_matrix(perm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    For a given permutation 'perm' of [0..n-1], return the\n",
    "    index array that can reorder the original flat vector\n",
    "    to match permuting the square matrix, then flattening.\n",
    "    \"\"\"\n",
    "    n = perm.size(0)\n",
    "    # 1) Build the lookup\n",
    "    index_matrix = build_upper_triangle_index_matrix(n)\n",
    "    # 2) Permute the lookup the same way as the data\n",
    "    perm_index_matrix = index_matrix[perm][:, perm]\n",
    "    # 3) Now flatten the upper triangle of this permuted matrix\n",
    "    r, c = torch.triu_indices(n, n, offset=1)\n",
    "    new_indices = perm_index_matrix[r, c]\n",
    "    return new_indices\n",
    "def square_to_flat(square_rdm: torch.Tensor) -> torch.Tensor:\n",
    "    # The same flattening order as build_upper_triangle_index_matrix\n",
    "    n_ = square_rdm.size(0)\n",
    "    r_, c_ = torch.triu_indices(n_, n_, offset=1)\n",
    "    return square_rdm[r_, c_]\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3, 13,  8, 19],\n",
       "        [ 3,  0,  2,  1,  4],\n",
       "        [13,  2,  0,  7, 14],\n",
       "        [ 8,  1,  7,  0,  9],\n",
       "        [19,  4, 14,  9,  0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n = 5\n",
    "matrix = torch.arange(n*n).reshape(n, n).triu(1)\n",
    "matrix = matrix + matrix.T\n",
    "matrix = matrix.fill_diagonal_(0)\n",
    "\n",
    "permutation = torch.randperm(n)\n",
    "\n",
    "matrix[permutation, :][:, permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 19, 19, 19, 19, 19, 14, 19,  9,  4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_permuted\n",
    "flat_permuted_via_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([81, 83, 85, 89, 84, 87, 86, 82, 80, 13, 15, 19, 14, 17, 16, 12, 10, 35,\n",
      "        39, 34, 37, 36, 32, 30, 59, 54, 57, 56, 52, 50, 94, 97, 96, 92, 90, 47,\n",
      "        46, 42, 40, 76, 72, 70, 62, 60, 20])\n",
      "tensor([18, 38, 58, 89, 48, 78, 68, 28,  8, 13, 15, 19, 14, 17, 16, 12,  1, 35,\n",
      "        39, 34, 37, 36, 23,  3, 59, 45, 57, 56, 25,  5, 49, 79, 69, 29,  9, 47,\n",
      "        46, 24,  4, 67, 27,  7, 26,  6,  2])\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "example_matrix = torch.arange(n*n).reshape(n, n)\n",
    "flat_example_matrix = square_to_flat(example_matrix)\n",
    "\n",
    "permutation = torch.randperm(n)\n",
    "\n",
    "\n",
    "\n",
    "# Permute the full matrix, then flatten\n",
    "permuted_matrix = example_matrix[permutation][:, permutation]\n",
    "flat_permuted = square_to_flat(permuted_matrix)\n",
    "\n",
    "# Directly apply the \"flat\" permutation\n",
    "flat_perm_indices = permutation_to_flat(permutation)\n",
    "flat_permuted_via_flat = flat_example_matrix[flat_perm_indices]\n",
    "\n",
    "# Check they match\n",
    "print(torch.allclose(flat_permuted, flat_permuted_via_flat))\n",
    "\n",
    "print(flat_permuted)\n",
    "\n",
    "print(flat_permuted_via_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "NOT EQUAL",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m flat_random_matrix \u001b[38;5;241m=\u001b[39m square_to_flat(random_matrix)\n\u001b[1;32m     12\u001b[0m flat_permuted_random_matrix_2 \u001b[38;5;241m=\u001b[39m flat_random_matrix[flat_permutation]\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (flat_permuted_random_matrix_2 \u001b[38;5;241m==\u001b[39m flat_permuted_random_matrix)\u001b[38;5;241m.\u001b[39mall()\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT EQUAL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: NOT EQUAL"
     ]
    }
   ],
   "source": [
    "permutation = torch.randperm(10)\n",
    "random_matrix = torch.arange(100).reshape(10, 10)\n",
    "\n",
    "\n",
    "# First method to permute and convert to flat\n",
    "permuted_random_matrix = random_matrix[permutation, :][:, permutation]\n",
    "flat_permuted_random_matrix = square_to_flat(permuted_random_matrix)\n",
    "\n",
    "# Second method\n",
    "flat_permutation = permutation_to_flat_perm(permutation)\n",
    "flat_random_matrix = square_to_flat(random_matrix)\n",
    "flat_permuted_random_matrix_2 = flat_random_matrix[flat_permutation]\n",
    "\n",
    "assert (flat_permuted_random_matrix_2 == flat_permuted_random_matrix).all().item(), \"NOT EQUAL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      " tensor([[     0,      0,      1,  ...,    746,    747,    748],\n",
      "        [     0,      0,    749,  ...,   1494,   1495,   1496],\n",
      "        [     1,    749,      0,  ...,   2241,   2242,   2243],\n",
      "        ...,\n",
      "        [   746,   1494,   2241,  ...,      0, 280872, 280873],\n",
      "        [   747,   1495,   2242,  ..., 280872,      0, 280874],\n",
      "        [   748,   1496,   2243,  ..., 280873, 280874,      0]])\n",
      "Flattened upper-tri: tensor([     0,      1,      2,  ..., 280872, 280873, 280874])\n",
      "Permutation (new order of original indices): tensor([ 66, 467,  32, 134,   2,  90,  53, 385, 371, 109, 147, 340, 464, 159,\n",
      "        563, 531, 382, 625, 267, 204, 338, 665,   5, 178, 503, 524, 237, 521,\n",
      "        597, 556, 717, 437, 137, 570, 651,  96,  84, 589, 458, 598, 393, 146,\n",
      "        553, 640, 522, 506,  78, 112, 601, 536,  77, 658, 350, 648, 525, 372,\n",
      "        662, 461, 688, 493,  54, 343,  56, 220, 257, 115, 315, 101, 205, 497,\n",
      "         22, 728, 446, 428, 180, 117, 124, 173, 449, 325, 473, 650, 327, 700,\n",
      "         16, 698, 488, 534, 304, 230,  68, 559, 613, 606, 547, 402,  59,  71,\n",
      "         23, 215, 678, 616, 576, 313, 116, 107,  79, 171, 331, 635, 481, 496,\n",
      "        390,   9, 443, 353,  52, 659, 161, 370, 381, 348, 113, 211, 552, 527,\n",
      "         82, 163, 226, 323, 633, 346,  34, 679, 145, 411, 462, 203, 278, 111,\n",
      "        508, 231, 361, 219, 716, 436, 472, 286, 345, 254, 483, 477, 579, 128,\n",
      "        546, 291, 482, 216, 292, 670, 588, 210, 369, 429, 636, 676, 729, 602,\n",
      "        438, 121, 708, 440, 637, 222, 746, 214, 103, 189, 297,  98, 695, 623,\n",
      "        352, 465,  35, 535, 692, 167, 609, 316, 713,  20, 326, 564, 654, 206,\n",
      "          7, 388, 243, 256, 484, 470, 681, 129,  10, 148, 151,  47,   3, 739,\n",
      "        310, 255, 463, 673, 720,  94, 294, 376, 156, 341, 628, 517, 283, 577,\n",
      "        694, 430,  64,  62, 277, 543, 172, 339, 191, 661, 182, 566, 672, 537,\n",
      "        193, 362, 329,  69, 224,  40, 135, 400, 240, 421, 288, 144, 406, 614,\n",
      "         97, 373, 366, 154, 562, 258, 509, 392, 399, 596, 723, 202, 639, 120,\n",
      "        557, 395, 184, 585, 427,  13,  70, 735, 363,  31, 732, 595, 308, 140,\n",
      "        271, 119, 646, 410, 260, 725, 301, 491, 387,  89, 550, 540, 554, 538,\n",
      "        618, 749, 126, 593, 478, 413, 684, 153, 719, 702,  58,  95,   0, 621,\n",
      "        355, 281, 118, 132, 697, 664, 586,  44, 630, 599, 199, 312, 187,  92,\n",
      "        378, 512, 186, 289,  93, 731, 469, 347,   1, 644, 320, 379, 248, 223,\n",
      "        264, 468, 384, 309, 742, 737, 354, 584, 643, 164, 434, 611,  28,  74,\n",
      "        207,  65, 627, 414, 528, 489, 667, 104, 587, 212, 736, 334, 296, 110,\n",
      "        142, 721, 228, 394, 632, 747, 401, 374, 701, 298, 235, 208, 409,  60,\n",
      "        337, 510, 603, 225, 567,   8, 360, 459, 612, 389, 244, 607, 273,  86,\n",
      "         36, 709, 740, 418, 452, 526, 529, 149, 480, 247, 250, 262, 441,  43,\n",
      "        133, 485, 498, 744, 333, 474, 516, 131, 604, 653, 317,  85, 127, 671,\n",
      "         26, 451, 295, 490, 150, 253,  39, 703, 285, 629, 306, 415, 290, 404,\n",
      "        359, 307, 106, 342, 375, 303,  27, 685, 460, 174, 265, 574, 416, 733,\n",
      "        580, 444, 190, 136,  11, 143, 397, 122,   6,  80, 495, 501, 229, 475,\n",
      "        405, 175, 551, 319,  33, 335, 626, 715, 311, 239, 649, 420, 233, 284,\n",
      "        471, 600, 454, 499, 141, 364, 573, 541, 100, 615, 419, 561, 200, 687,\n",
      "        249, 270,  25, 634, 344, 188, 730, 505, 519, 357,  75, 169, 504, 572,\n",
      "        105, 234, 560, 422, 195, 166, 324,  67, 102, 209, 631,  88, 287, 677,\n",
      "        530,  12, 158, 274, 165,  50, 279, 502, 605, 442, 245, 266, 391, 617,\n",
      "        710, 718, 727, 280, 487, 578,  83, 569, 368, 513, 641,  46, 380, 507,\n",
      "         81, 652, 722, 236, 457, 523, 108, 704,  55, 549, 686, 302, 160,  37,\n",
      "        130, 683, 624, 213, 426, 583, 638, 152, 571, 157, 177, 456, 268, 476,\n",
      "        500, 741, 275, 269,  21, 439,  18, 745, 666, 660, 221, 645, 396,  91,\n",
      "        238, 707, 515, 455, 590, 575, 232, 591, 299, 545, 656, 181, 558,  45,\n",
      "        201, 748, 407, 542, 305, 217, 349, 431,  30, 162, 259, 743, 176, 351,\n",
      "        123, 544, 227, 125, 185, 433, 712, 170, 196,  48, 669, 568, 332, 453,\n",
      "        674, 261, 479, 435, 494,  51, 367, 300, 726, 734, 192, 424, 241, 377,\n",
      "        445, 293, 696, 383, 520,  14, 272, 179, 693, 466, 690, 620, 197, 168,\n",
      "        251, 532,  73, 691, 314, 183, 514, 511, 423, 450,  72,  41,  57, 533,\n",
      "        714, 398,  42, 386, 642, 657, 330, 548, 114, 492, 321, 711, 194, 738,\n",
      "        622, 539, 610, 322, 448,  15, 408, 328, 403,   4, 581, 682, 218, 155,\n",
      "        565, 689, 447, 668, 198, 425, 138, 263,  99, 699, 252, 318,  49,  24,\n",
      "        276, 663, 706, 432, 417,  61, 246,  29,  76, 486, 608,  17, 675, 592,\n",
      "        518, 365, 555, 336,  38, 582,  63, 619, 139, 724, 282, 242, 594, 412,\n",
      "         19, 655, 647,  87, 358, 680, 356, 705])\n",
      "Permuted matrix flattened: tensor([ 47689,  23505,  47356,  ..., 203777, 278484, 203802])\n",
      "Reordered flat directly: tensor([ 47689,  23505,  47356,  ..., 203777, 278484, 203802])\n",
      "Matches? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def square_to_flat(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given an n x n PyTorch tensor (symmetric matrix),\n",
    "    return the flattened upper triangle (excluding the diagonal).\n",
    "\n",
    "    The returned 1D tensor has length n*(n-1)//2 and\n",
    "    follows the order of torch.triu_indices(n, n, offset=1).\n",
    "    \"\"\"\n",
    "    n = matrix.shape[0]\n",
    "    i, j = torch.triu_indices(n, n, offset=1)  # shape (m, ), m=n*(n-1)//2\n",
    "    return matrix[i, j]\n",
    "\n",
    "def permutation_to_flat(perm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a permutation 'perm' of size n that is used to reorder\n",
    "    both rows and columns of an n x n matrix, return the index\n",
    "    array (length n*(n-1)//2) that can reorder the flat upper-triangle\n",
    "    to produce the same result as permuting the matrix first, then flattening.\n",
    "\n",
    "    Usage:\n",
    "        flat_reordered = flat_original[ new_indices ]\n",
    "\n",
    "    so that 'flat_reordered' matches:\n",
    "        square_to_flat( matrix[perm][:, perm] ).\n",
    "    \"\"\"\n",
    "    n = perm.numel()\n",
    "    # Compute inverse permutation (map old_index -> new_index)\n",
    "    # so if perm[new_idx] = old_idx, then invPerm[old_idx] = new_idx\n",
    "    invPerm = torch.empty_like(perm)\n",
    "    invPerm[perm] = torch.arange(n, device=perm.device)\n",
    "\n",
    "    # Original upper-tri pairs\n",
    "    i, j = torch.triu_indices(n, n, offset=1)  # shape: (m,)\n",
    "    # Apply inverse perm to each coordinate\n",
    "    new_i = invPerm[i]\n",
    "    new_j = invPerm[j]\n",
    "    # Enforce new_i < new_j\n",
    "    lower = torch.min(new_i, new_j)\n",
    "    upper = torch.max(new_i, new_j)\n",
    "\n",
    "    # Convert (lower, upper) to new flat index\n",
    "    # Formula consistent with torch.triu_indices(n, n, offset=1) ordering\n",
    "    new_indices = ((n * (n - 1)) // 2) \\\n",
    "                  - ((n - lower) * (n - lower - 1)) // 2 \\\n",
    "                  + (upper - lower - 1)\n",
    "\n",
    "    # new_indices tells us \"old -> new\" positions, but we typically\n",
    "    # want an array 'order' s.t. flat_reordered = flat_orig[order].\n",
    "    # That means 'order[new_pos] = old_pos'. So do argsort:\n",
    "    order = torch.argsort(new_indices)\n",
    "    return order\n",
    "\n",
    "def permutation_to_flat_no_sort(perm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Same logic as before, but we skip argsort by building\n",
    "    the 'order' (new->old) array in O(N) time.\n",
    "    \"\"\"\n",
    "    n = perm.numel()\n",
    "    device = perm.device\n",
    "\n",
    "    # Inverse permutation\n",
    "    invPerm = torch.empty_like(perm)\n",
    "    invPerm[perm] = torch.arange(n, device=device)\n",
    "\n",
    "    # Original upper-tri indices\n",
    "    i, j = torch.triu_indices(n, n, offset=1, device=device)\n",
    "    new_i = invPerm[i]\n",
    "    new_j = invPerm[j]\n",
    "\n",
    "    # Ensure new_i < new_j\n",
    "    lower = torch.min(new_i, new_j)\n",
    "    upper = torch.max(new_i, new_j)\n",
    "\n",
    "    # \"old -> new\" position\n",
    "    new_indices = ((n*(n-1))//2\n",
    "                   - ((n - lower)*(n - lower - 1))//2\n",
    "                   + (upper - lower - 1))\n",
    "\n",
    "    # Build the \"new -> old\" in O(N) with direct indexing\n",
    "    # length = n*(n-1)//2\n",
    "    N = new_indices.numel()\n",
    "    order = torch.empty(N, dtype=torch.long, device=device)\n",
    "    # for each old index k, new_indices[k] = new position\n",
    "    # so we do order[new_pos] = old_pos\n",
    "    old_positions = torch.arange(N, device=device)\n",
    "    order[new_indices] = old_positions\n",
    "\n",
    "    return order\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    n = 750\n",
    "    # Create an n x n matrix where the upper triangle holds distinct values\n",
    "    M = torch.zeros(n, n, dtype=torch.long)\n",
    "    idx_upper_i, idx_upper_j = torch.triu_indices(n, n, offset=1)\n",
    "    for k in range(idx_upper_i.numel()):\n",
    "        i_ = idx_upper_i[k]\n",
    "        j_ = idx_upper_j[k]\n",
    "        # Assign the flat index k to positions (i_, j_) and (j_, i_)\n",
    "        M[i_, j_] = k\n",
    "        M[j_, i_] = k\n",
    "\n",
    "    print(\"Original matrix:\\n\", M)\n",
    "\n",
    "    # Flatten the upper triangle\n",
    "    orig_flat = square_to_flat(M)\n",
    "    print(\"Flattened upper-tri:\", orig_flat)\n",
    "\n",
    "    # Example permutation\n",
    "    perm = torch.randperm(n)\n",
    "    print(\"Permutation (new order of original indices):\", perm)\n",
    "\n",
    "    # Permute the full matrix, then flatten\n",
    "    M_perm = M[perm][:, perm]\n",
    "    new_flat = square_to_flat(M_perm)\n",
    "    print(\"Permuted matrix flattened:\", new_flat)\n",
    "\n",
    "    # Compute the flat permutation indices to reorder the original flat vector\n",
    "    flat_order = permutation_to_flat_no_sort(perm)\n",
    "    flat_reordered = orig_flat[flat_order]\n",
    "    print(\"Reordered flat directly:\", flat_reordered)\n",
    "\n",
    "    # Confirm both approaches match\n",
    "    print(\"Matches?\", torch.allclose(new_flat, flat_reordered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37, 57, 17, 78, 27, 47,  7, 67, 79, 35, 13, 38, 23, 34,  3, 36, 39, 15,\n",
       "        58, 25, 45,  5, 56, 59, 18, 12, 14,  1, 16, 19, 28, 48,  8, 68, 89, 24,\n",
       "         2, 26, 29,  4, 46, 49,  6,  9, 69])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_permuted_random_matrix\n",
    "flat_permuted_random_matrix_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      " [[0 0 1 2]\n",
      " [0 0 3 4]\n",
      " [1 3 0 5]\n",
      " [2 4 5 0]]\n",
      "Flattened (upper-tri): [0 1 2 3 4 5]\n",
      "Permutation: [2 0 1 3]\n",
      "Permuted matrix flattened: [1 3 5 0 2 4]\n",
      "Reordered flat directly: [1 3 5 0 2 4]\n",
      "Matches? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def square_to_flat(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given an n x n square (symmetric) matrix,\n",
    "    return the flattened upper triangle (excluding the diagonal).\n",
    "\n",
    "    The returned vector has length n*(n-1)//2 and uses\n",
    "    the same ordering as np.triu_indices(n, k=1).\n",
    "    \"\"\"\n",
    "    n = matrix.shape[0]\n",
    "    i, j = np.triu_indices(n, k=1)\n",
    "    return matrix[i, j]\n",
    "\n",
    "def permutation_to_flat(perm: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a permutation 'perm' of [0..n-1] that reorders rows and columns\n",
    "    of an n x n matrix, return the corresponding permutation of the\n",
    "    flattened upper-triangle indices (length n*(n-1)//2), so that:\n",
    "\n",
    "       flat_permuted = flat_original[ new_indices ]\n",
    "\n",
    "    is equivalent to permuting the full matrix first and then flattening.\n",
    "    \"\"\"\n",
    "    # size\n",
    "    n = len(perm)\n",
    "    # Inverse permutation (maps original index -> new index)\n",
    "    invPerm = np.argsort(perm)\n",
    "\n",
    "    # Original upper-tri pairs\n",
    "    i, j = np.triu_indices(n, k=1)  # shape (m,), m = n*(n-1)//2\n",
    "\n",
    "    # Apply inverse perm to each coordinate\n",
    "    new_i = invPerm[i]\n",
    "    new_j = invPerm[j]\n",
    "\n",
    "    # Ensure new_i < new_j (upper-tri)\n",
    "    new_i, new_j = np.minimum(new_i, new_j), np.maximum(new_i, new_j)\n",
    "\n",
    "    # Convert (new_i, new_j) to new flat index\n",
    "    # The formula consistent with the ordering from np.triu_indices(n, k=1)\n",
    "    new_indices = ((n * (n - 1)) // 2) \\\n",
    "                  - ((n - new_i) * (n - new_i - 1)) // 2 \\\n",
    "                  + (new_j - new_i - 1)\n",
    "\n",
    "    # new_indices tells where each original flat index should go.\n",
    "    # But typically we want the \"inverse\" permutation so that if we do\n",
    "    #   flat_permuted = flat_original[some_indices]\n",
    "    # we get the correct effect.\n",
    "    #\n",
    "    # The simplest approach: new_indices is \"old -> new\".  So we do an argsort\n",
    "    # to get \"new -> old\", i.e. the array that we can use to reorder the original flat\n",
    "    order = np.argsort(new_indices)\n",
    "\n",
    "    return order\n",
    "\n",
    "# ------------------------------\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    n = 4\n",
    "    M = np.zeros((n, n), int)\n",
    "    i_up, j_up = np.triu_indices(n, k=1)\n",
    "    for k, (r, c) in enumerate(zip(i_up, j_up)):\n",
    "        M[r, c] = k\n",
    "        M[c, r] = k\n",
    "\n",
    "    print(\"Original matrix:\\n\", M)\n",
    "    orig_flat = square_to_flat(M)\n",
    "    print(\"Flattened (upper-tri):\", orig_flat)\n",
    "\n",
    "    # Example permutation\n",
    "    \n",
    "    print(\"Permutation:\", perm)\n",
    "\n",
    "    # Permute the full matrix, then flatten\n",
    "    M_perm = M[perm][:, perm]\n",
    "    new_flat = square_to_flat(M_perm)\n",
    "    print(\"Permuted matrix flattened:\", new_flat)\n",
    "\n",
    "    # Compute the flat permutation indices\n",
    "    flat_order = permutation_to_flat(perm)\n",
    "    flat_reordered = orig_flat[flat_order]\n",
    "    print(\"Reordered flat directly:\", flat_reordered)\n",
    "\n",
    "    # They should match\n",
    "    print(\"Matches?\", np.allclose(new_flat, flat_reordered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched order shape: torch.Size([2, 10])\n",
      "Original matrix:\n",
      " tensor([[0, 0, 1, 2, 3],\n",
      "        [0, 0, 4, 5, 6],\n",
      "        [1, 4, 0, 7, 8],\n",
      "        [2, 5, 7, 0, 9],\n",
      "        [3, 6, 8, 9, 0]])\n",
      "Original flat: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Permutation #0 flattened: tensor([5, 9, 2, 7, 6, 0, 4, 3, 8, 1])\n",
      "Reordered flat #0: tensor([5, 9, 2, 7, 6, 0, 4, 3, 8, 1])\n",
      "Matches #0? True\n",
      "Permutation #1 flattened: tensor([4, 5, 6, 0, 7, 8, 1, 9, 2, 3])\n",
      "Reordered flat #1: tensor([4, 5, 6, 0, 7, 8, 1, 9, 2, 3])\n",
      "Matches #1? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def batched_flat_permutation_no_sort(perm_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a batch of permutations of shape (n_perms, n),\n",
    "    compute the corresponding 'flat permutation' of length N = n*(n-1)//2\n",
    "    for each permutation. The result is an integer tensor of shape (n_perms, N).\n",
    "\n",
    "    Specifically:\n",
    "      - perm_batch[p] is a permutation of [0..n-1].\n",
    "      - The returned 'order' has the same shape (n_perms, N).\n",
    "      - For each p, order[p] is a 1D index array of length N such that:\n",
    "            flat_reordered = flat_original[ order[p] ]\n",
    "        matches flattening the matrix after applying perm to rows/columns.\n",
    "\n",
    "    This version avoids any O(N log N) sorting by building 'new->old' in O(N) time.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 0) Setup and shapes\n",
    "    n_perms, n = perm_batch.shape\n",
    "    device = perm_batch.device\n",
    "    N = n * (n - 1) // 2  # number of upper-tri (excl. diagonal) entries\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 1) Build inverse permutation for each row in a single vectorized pass\n",
    "    #    - invPerm[p, :] is the inverse of perm_batch[p, :]\n",
    "    #    - \"scatter\" approach: for each p, for each k in [0..n-1],\n",
    "    #         invPerm[p, perm_batch[p,k]] = k\n",
    "    invPerm = torch.empty_like(perm_batch)\n",
    "    # Create row indices and col indices for assignment\n",
    "    rows = torch.arange(n_perms, device=device).unsqueeze(1).expand(-1, n)  # (n_perms, n)\n",
    "    cols = perm_batch  # shape (n_perms, n)\n",
    "    vals = torch.arange(n, device=device).unsqueeze(0).expand(n_perms, -1)  # (n_perms, n)\n",
    "    invPerm[rows, cols] = vals  # fill inverse permutations\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 2) Precompute all (i, j) for the upper triangle (i<j)\n",
    "    #    We'll broadcast these across permutations in a vectorized manner\n",
    "    i_j = torch.triu_indices(n, n, offset=1, device=device)  # shape (2, N)\n",
    "    i, j = i_j[0], i_j[1]  # each shape (N,)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 3) Map (i, j) -> (new_i, new_j) via inverse permutation, for all p\n",
    "    #    new_i[p, k] = invPerm[p, i[k]]\n",
    "    #    new_j[p, k] = invPerm[p, j[k]]\n",
    "    #    shape => (n_perms, N)\n",
    "    new_i = invPerm[:, i]  # (n_perms, N)\n",
    "    new_j = invPerm[:, j]  # (n_perms, N)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 4) Ensure new_i < new_j\n",
    "    lower = torch.min(new_i, new_j)\n",
    "    upper = torch.max(new_i, new_j)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 5) Compute \"old->new\" positions:\n",
    "    #    new_indices[p, k] = new position of the old index k in flat space\n",
    "    #    The formula consistent with torch.triu_indices(n,n,offset=1):\n",
    "    #\n",
    "    #       new_idx = N - ((n-lower)*(n-lower-1))//2 + (upper - lower - 1)\n",
    "    #\n",
    "    #    This yields shape (n_perms, N).\n",
    "    tmp = (n - lower) * (n - lower - 1) // 2\n",
    "    new_indices = (N - tmp) + (upper - lower - 1)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 6) Build the final \"new->old\" order in O(N) time (no sorting).\n",
    "    #    For each row p, for each old index k, we do:\n",
    "    #        new_pos = new_indices[p, k]\n",
    "    #        order[p, new_pos] = k\n",
    "    #    We'll do this with advanced indexing in one shot.\n",
    "    order = torch.empty_like(new_indices)  # (n_perms, N)\n",
    "    old_positions = torch.arange(N, device=device).unsqueeze(0).expand(n_perms, -1)  # shape (n_perms, N)\n",
    "\n",
    "    # row indices => shape (n_perms, N)\n",
    "    rows_big = torch.arange(n_perms, device=device).unsqueeze(1).expand(-1, N)\n",
    "    # col indices => new_indices, shape (n_perms, N)\n",
    "\n",
    "    order[rows_big, new_indices] = old_positions\n",
    "\n",
    "    return order\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DEMO with smaller scale\n",
    "if __name__ == \"__main__\":\n",
    "    n_perms = 2\n",
    "    n = 5\n",
    "    # Example permutations\n",
    "    perm_batch = torch.tensor([\n",
    "        [3, 1, 4, 0, 2],  # each row is a permutation of [0..4]\n",
    "        [1, 2, 3, 4, 0],\n",
    "    ], dtype=torch.long, device='cpu')\n",
    "\n",
    "    # Get the batched index reorder\n",
    "    order_batched = batched_flat_permutation_no_sort(perm_batch)\n",
    "    print(\"Batched order shape:\", order_batched.shape)  # (2, n*(n-1)//2) = (2,10)\n",
    "\n",
    "    # Let's verify correctness for the first permutation\n",
    "    # 1) Build a test matrix M with distinct upper-tri entries\n",
    "    M = torch.zeros(n, n, dtype=torch.long)\n",
    "    i_up, j_up = torch.triu_indices(n, n, offset=1)\n",
    "    for idx in range(i_up.numel()):\n",
    "        M[i_up[idx], j_up[idx]] = idx\n",
    "        M[j_up[idx], i_up[idx]] = idx\n",
    "    print(\"Original matrix:\\n\", M)\n",
    "\n",
    "    # Flatten upper triangle\n",
    "    orig_flat = M[i_up, j_up]\n",
    "    print(\"Original flat:\", orig_flat)\n",
    "\n",
    "    # Permute rows/cols with perm_batch[0] -> flatten\n",
    "    M0 = M[perm_batch[0]][:, perm_batch[0]]\n",
    "    perm_flat = M0[i_up, j_up]\n",
    "    print(\"Permutation #0 flattened:\", perm_flat)\n",
    "\n",
    "    # Reorder original flat directly\n",
    "    order_0 = order_batched[0]\n",
    "    flat_0 = orig_flat[order_0]\n",
    "    print(\"Reordered flat #0:\", flat_0)\n",
    "    print(\"Matches #0?\", torch.allclose(perm_flat, flat_0))\n",
    "\n",
    "    # Similarly check permutation #1\n",
    "    M1 = M[perm_batch[1]][:, perm_batch[1]]\n",
    "    perm_flat_1 = M1[i_up, j_up]\n",
    "    order_1 = order_batched[1]\n",
    "    flat_1 = orig_flat[order_1]\n",
    "    print(\"Permutation #1 flattened:\", perm_flat_1)\n",
    "    print(\"Reordered flat #1:\", flat_1)\n",
    "    print(\"Matches #1?\", torch.allclose(perm_flat_1, flat_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_batched.shape: torch.Size([2, 10])\n",
      "Matches perm #0? True\n",
      "Matches perm #1? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def batched_flat_permutation_no_sort(perm_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a batch of permutations of shape (n_perms, n), compute\n",
    "    the 'flat-permutation' index array of shape (n_perms, N), where\n",
    "    N = n*(n-1)//2.\n",
    "\n",
    "    - perm_batch[p] is a permutation of [0..n-1].\n",
    "    - The returned array, say 'order', satisfies:\n",
    "         flat_reordered = flat_original[ order[p] ]\n",
    "      which is equivalent to:\n",
    "         1) reshaping flat_original back to (n x n) (upper tri only)\n",
    "         2) permuting rows/columns of that n x n with perm_batch[p]\n",
    "         3) flattening the upper triangle again.\n",
    "    \n",
    "    This version avoids an O(N log N) sort, instead building the\n",
    "    new->old mapping in O(N) time for each permutation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shape info\n",
    "    n_perms, n = perm_batch.shape\n",
    "    device = perm_batch.device\n",
    "    # Number of strictly upper-triangle entries\n",
    "    N = n * (n - 1) // 2\n",
    "\n",
    "    # 1) Build inverse permutation for each p in perm_batch\n",
    "    #    invPerm[p, perm_batch[p, k]] = k\n",
    "    invPerm = torch.empty_like(perm_batch)\n",
    "    rows = torch.arange(n_perms, device=device).unsqueeze(1).expand(-1, n)  # (n_perms, n)\n",
    "    cols = perm_batch\n",
    "    vals = torch.arange(n, device=device).unsqueeze(0).expand(n_perms, -1)  # (n_perms, n)\n",
    "    invPerm[rows, cols] = vals\n",
    "\n",
    "    # 2) Precompute all (i, j) for upper triangle (i < j)\n",
    "    i_j = torch.triu_indices(n, n, offset=1, device=device)\n",
    "    i, j = i_j[0], i_j[1]  # each shape (N,)\n",
    "\n",
    "    # 3) For each permutation p, map (i, j) -> (new_i, new_j)\n",
    "    new_i = invPerm[:, i]  # (n_perms, N)\n",
    "    new_j = invPerm[:, j]  # (n_perms, N)\n",
    "\n",
    "    # 4) Ensure new_i < new_j\n",
    "    lower = torch.min(new_i, new_j)\n",
    "    upper = torch.max(new_i, new_j)\n",
    "\n",
    "    # 5) \"old->new\" positions: new_indices[p, k] = new position of old index k\n",
    "    tmp = (n - lower) * (n - lower - 1) // 2\n",
    "    new_indices = (N - tmp) + (upper - lower - 1)  # shape (n_perms, N)\n",
    "\n",
    "    # 6) Build \"new->old\" order in O(N) time (no sort):\n",
    "    #    order[p, new_pos] = old_pos\n",
    "    order = torch.empty_like(new_indices)\n",
    "    old_positions = torch.arange(N, device=device).unsqueeze(0).expand(n_perms, -1)  # (n_perms, N)\n",
    "    row_ids = torch.arange(n_perms, device=device).unsqueeze(1).expand(-1, N)        # (n_perms, N)\n",
    "\n",
    "    order[row_ids, new_indices] = old_positions  # scatter: new->old\n",
    "\n",
    "    return order\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    n_perms = 2\n",
    "    n = 5\n",
    "    # Sample permutations\n",
    "    perm_batch = torch.tensor([\n",
    "        [3, 1, 4, 0, 2],\n",
    "        [1, 2, 3, 4, 0],\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    order_batched = batched_flat_permutation_no_sort(perm_batch)\n",
    "    print(\"order_batched.shape:\", order_batched.shape)  # => (2, 10) for n=5\n",
    "\n",
    "    # Demo for a single matrix M with distinct upper-tri values\n",
    "    M = torch.zeros(n, n, dtype=torch.long)\n",
    "    i_up, j_up = torch.triu_indices(n, n, offset=1)\n",
    "    for k in range(i_up.numel()):\n",
    "        r, c = i_up[k], j_up[k]\n",
    "        M[r, c] = k\n",
    "        M[c, r] = k\n",
    "\n",
    "    # Flatten M\n",
    "    orig_flat = M[i_up, j_up]\n",
    "\n",
    "    # Perm #0\n",
    "    perm0 = perm_batch[0]\n",
    "    M0 = M[perm0][:, perm0]\n",
    "    perm0_flat = M0[i_up, j_up]\n",
    "    direct0 = orig_flat[order_batched[0]]\n",
    "    print(\"Matches perm #0?\", torch.allclose(perm0_flat, direct0))\n",
    "\n",
    "    # Perm #1\n",
    "    perm1 = perm_batch[1]\n",
    "    M1 = M[perm1][:, perm1]\n",
    "    perm1_flat = M1[i_up, j_up]\n",
    "    direct1 = orig_flat[order_batched[1]]\n",
    "    print(\"Matches perm #1?\", torch.allclose(perm1_flat, direct1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
